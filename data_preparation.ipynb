{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "962e353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import os # for interacting with the operating system\n",
    "import numpy as np # for linear algebra operations\n",
    "import pandas as pd # for data processing and reading/writing CSV files\n",
    "import matplotlib.pyplot as plt # for plotting data\n",
    "import seaborn as sns # for creating beautiful plots\n",
    "import librosa # a library for analyzing audio and music\n",
    "import librosa.display # for displaying audio data\n",
    "from IPython.display import Audio # for playing audio files\n",
    "\n",
    "# Importing necessary modules from scikit-learn library\n",
    "# StandardScaler is used for feature scaling\n",
    "# OneHotEncoder is used for one-hot encoding categorical variables\n",
    "# confusion_matrix and classification_report are used for evaluating the performance of a classifier\n",
    "# train_test_split is used for splitting the data into training and testing sets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, AveragePooling1D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, AveragePooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c699754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(female_X, female_Y, male_X, male_Y,Females_Features, Males_Features, encoder):\n",
    "    # Splitting the data into feature and target variables for female dataset\n",
    "    # Features are stored in female_X by excluding the last column from the Females_Features dataframe\n",
    "    # The target variable, 'labels', is stored in female_Y\n",
    "    female_X = Females_Features.iloc[: ,:-1].values\n",
    "    female_Y = Females_Features['labels'].values\n",
    "\n",
    "    # Splitting the data into feature and target variables for male dataset\n",
    "    # Features are stored in male_X by excluding the last column from the Males_Features dataframe\n",
    "    # The target variable, 'labels', is stored in male_Y\n",
    "    male_X = Males_Features.iloc[: ,:-1].values\n",
    "    male_Y = Males_Features['labels'].values\n",
    "    \n",
    "\n",
    "    # One-hot encoding the female target variable and converting it to an array\n",
    "    # The target variable is first converted to a numpy array and reshaped to have only one column\n",
    "    # The reshaped array is then fit and transformed using the OneHotEncoder instance\n",
    "    female_Y = encoder.fit_transform(np.array(female_Y).reshape(-1,1)).toarray()\n",
    "\n",
    "    # One-hot encoding the male target variable and converting it to an array\n",
    "    # The target variable is first converted to a numpy array and reshaped to have only one column\n",
    "    # The reshaped array is then fit and transformed using the OneHotEncoder instance\n",
    "    male_Y = encoder.fit_transform(np.array(male_Y).reshape(-1,1)).toarray()\n",
    "    \n",
    "    return female_X, female_Y, male_X, male_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80c50a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(female_X, female_Y, male_X, male_Y):\n",
    "    # Concatenating the features and targets of female and male datasets to create a combined dataset\n",
    "    no_gender_X = np.concatenate((female_X, male_X))\n",
    "    no_gender_Y = np.concatenate((female_Y, male_Y))\n",
    "\n",
    "    # Splitting the combined dataset into training and testing sets\n",
    "    # 20% of the data is set aside for testing and the rest for training\n",
    "    # The data is shuffled before the split to ensure randomness\n",
    "    x_train, x_test, y_train, y_test = train_test_split(no_gender_X, no_gender_Y, random_state=0, test_size=0.20, shuffle=True)\n",
    "\n",
    "    # Printing the shape of the training and testing sets\n",
    "    print('shape of the training sets: x_train: ',x_train.shape,' y_train: ', y_train.shape,'\\nshape of the testing sets: x_test: ', x_test.shape,' y_test: ', y_test.shape)\n",
    "    \n",
    "    # Create an instance of the StandardScaler class\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit and transform the training data using the StandardScaler object\n",
    "    # The fit_transform method calculates the mean and standard deviation of the input data and then scales the data accordingly\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "\n",
    "    # Apply the scaling calculated from the training data to the test data\n",
    "    # The transform method uses the mean and standard deviation calculated from the training data to scale the test data\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec4f4470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(x_train, x_test, y_train, y_test):\n",
    "    # Add an extra dimension to the training data along the specified axis (axis=2)\n",
    "    # This is to ensure that the data has the correct shape for input into a neural network\n",
    "    x_train = np.expand_dims(x_train, axis=2)\n",
    "\n",
    "    # Add an extra dimension to the test data along the specified axis (axis=2)\n",
    "    x_test = np.expand_dims(x_test, axis=2)\n",
    "    \n",
    "    # Printing the shape of the training and testing sets\n",
    "    print('shape of the training sets: x_train: ',x_train.shape,' y_train: ', y_train.shape,'\\nshape of the testing sets: x_test: ', x_test.shape,' y_test: ', y_test.shape)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcc0602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
