{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a6f4e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4ca4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model architecture from JSON file\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load the saved model weights from H5 file\n",
    "model.load_weights('model.h5')\n",
    "# Load the model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfa28b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add noise to data\n",
    "def add_noise(data):\n",
    "    # Calculate noise amplitude using uniform random value between 0 and 0.04 times the maximum value of data\n",
    "    noise_amplitude = 0.04*np.random.uniform()*np.amax(data)\n",
    "    # Add the calculated noise to the data\n",
    "    data = data + noise_amplitude*np.random.normal(size=data.shape[0])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06188b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to shift the data\n",
    "def shift(data):\n",
    "    # Calculate the shift range using uniform random value between -5 and 5 multiplied by 1000\n",
    "    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n",
    "    # Shift the data using numpy's roll function\n",
    "    return np.roll(data, shift_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afbb0300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to increase the speed of the data\n",
    "def increase_speed(data, speed_factor = 1.25):\n",
    "    # Increase the speed of the data using librosa's time_stretch function\n",
    "    return librosa.effects.time_stretch(data, speed_factor)\n",
    "\n",
    "# Function to decrease the speed of the data\n",
    "def decrease_speed(data, speed_factor = 0.75):\n",
    "    # Decrease the speed of the data using librosa's time_stretch function\n",
    "    return librosa.effects.time_stretch(data, speed_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7a38fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to stretch the data\n",
    "def stretch(data, rate=0.70):\n",
    "    # Stretch the data using librosa's time_stretch function\n",
    "    return librosa.effects.time_stretch(data, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c089d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to change the pitch of the data\n",
    "def change_pitch(data, sampling_rate, pitch_factor=0.8):\n",
    "    # Change the pitch of the data using librosa's pitch_shift function\n",
    "    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf532bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to extract features from the audio file\n",
    "def extract_features(data):\n",
    "    result = np.array([])\n",
    "    \n",
    "    # Compute the Mel-frequency cepstral coefficients (MFCCs)\n",
    "    # Use 58 MFCCs to get ~60 ms frames\n",
    "    mfccs = librosa.feature.mfcc(y=data, sr=22050, n_mfcc=58)\n",
    "    \n",
    "    # Compute the average MFCCs for each frame\n",
    "    mfccs_processed = np.mean(mfccs.T,axis=0)\n",
    "    \n",
    "    # Add the processed MFCCs to the result array\n",
    "    result = np.array(mfccs_processed)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e426532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to get features for a given audio file\n",
    "def get_features(path):\n",
    "    # Load the audio file, taking care of the no audio at the start and end of the file\n",
    "    data, sample_rate = librosa.load(path, duration=3, offset=0.5, res_type='kaiser_fast') \n",
    "    \n",
    "    # Extract features without augmentation\n",
    "    result_1 = extract_features(data)\n",
    "    result = np.array(result_1)\n",
    "    \n",
    "    # Extract features with added noise\n",
    "    noise_data = add_noise(data)\n",
    "    result_2 = extract_features(noise_data)\n",
    "    result = np.vstack((result, result_2)) # Stack the results vertically\n",
    "    \n",
    "    # Extract features with time stretching\n",
    "    stretch_data = stretch(data)\n",
    "    result_3 = extract_features(stretch_data)\n",
    "    result = np.vstack((result, result_3))\n",
    "    \n",
    "    # Extract features with time shifting\n",
    "    shift_data = shift(data)\n",
    "    result_4 = extract_features(shift_data)\n",
    "    result = np.vstack((result, result_4))\n",
    "    \n",
    "    # Extract features with pitch shifting\n",
    "    pitch_data = change_pitch(data, sample_rate)\n",
    "    result_5 = extract_features(pitch_data)\n",
    "    result = np.vstack((result, result_5)) \n",
    "    \n",
    "    # Extract features with increased speed\n",
    "    higher_speed_data = increase_speed(data)\n",
    "    result_6 = extract_features(higher_speed_data)\n",
    "    result = np.vstack((result, result_6))\n",
    "    \n",
    "    # Extract features with decreased speed\n",
    "    lower_speed_data = decrease_speed(data)\n",
    "    result_7 = extract_features(lower_speed_data)\n",
    "    result = np.vstack((result, result_7))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a597caef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make a prediction for a given audio file\n",
    "def predict_emotion(file_path):\n",
    "    # Get the features for the audio file\n",
    "    features = get_features(file_path)\n",
    "\n",
    "    # Normalize the features\n",
    "    features = (features - np.mean(features)) / np.std(features)\n",
    "\n",
    "    # Add an extra dimension to match the input shape of the model\n",
    "    features = np.expand_dims(features, axis=0)\n",
    "\n",
    "    # Make the prediction using the trained model\n",
    "    prediction = model.predict(features)\n",
    "\n",
    "    # Get the predicted label\n",
    "    label = np.argmax(prediction)\n",
    "\n",
    "    # Return the predicted emotion label\n",
    "    if label == 0:\n",
    "        return 'neutral'\n",
    "    elif label == 1:\n",
    "        return 'calm'\n",
    "    elif label == 2:\n",
    "        return 'happy'\n",
    "    elif label == 3:\n",
    "        return 'sad'\n",
    "    elif label == 4:\n",
    "        return 'angry'\n",
    "    elif label == 5:\n",
    "        return 'fear'\n",
    "    elif label == 6:\n",
    "        return 'disgust'\n",
    "    else:\n",
    "        return 'surprise'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57ed48b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_3700\\2036098180.py:4: FutureWarning: Pass rate=0.7 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, rate)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_3700\\3778454608.py:4: FutureWarning: Pass sr=22050, n_steps=0.8 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_3700\\1913875087.py:4: FutureWarning: Pass rate=1.25 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, speed_factor)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_3700\\1913875087.py:9: FutureWarning: Pass rate=0.75 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, speed_factor)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 58, 1), found shape=(None, 7, 58)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3700\\208403709.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredict_emotion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:\\\\final_voice_model_3\\\\RAVDESS\\\\Actor_01\\\\03-01-01-01-01-01-01.wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3700\\1870736533.py\u001b[0m in \u001b[0;36mpredict_emotion\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# Make the prediction using the trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Get the predicted label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                     \u001b[0mretval_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mld\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m                 \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 58, 1), found shape=(None, 7, 58)\n"
     ]
    }
   ],
   "source": [
    "print(predict_emotion(\"D:\\\\final_voice_model_3\\\\RAVDESS\\\\Actor_01\\\\03-01-01-01-01-01-01.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58fe416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to predict the mood of an audio file\n",
    "def predict_mood(file_path):\n",
    "    # Load the audio file, taking care of the no audio at the start and end of the file\n",
    "    data, sample_rate = librosa.load(file_path, duration=3, offset=0.5, res_type='kaiser_fast') \n",
    "    # Get the features of the audio file\n",
    "    features = extract_features(data)\n",
    "    \n",
    "    # Reshape the features to match the input shape of the model\n",
    "    features = features.reshape(features.shape[0], features.shape[1], 1)\n",
    "    \n",
    "    # Normalize the features\n",
    "    features = features / np.max(features)\n",
    "    \n",
    "    # Use the model to predict the mood of the audio file\n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    # Get the predicted emotion\n",
    "    emotions = np.argmax(predictions)\n",
    "\n",
    "    return emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2ae3410",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3700\\1240455513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memotions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredict_mood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"D:\\\\final_voice_model_3\\\\RAVDESS\\\\Actor_01\\\\03-01-01-01-01-01-01.wav\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3700\\2480459151.py\u001b[0m in \u001b[0;36mpredict_mood\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Reshape the features to match the input shape of the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Normalize the features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "print(emotions[predict_mood(\"D:\\\\final_voice_model_3\\\\RAVDESS\\\\Actor_01\\\\03-01-01-01-01-01-01.wav\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d019adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to import .ipynb files\n",
    "import import_ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f99bab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_audio_features(audio_path,sampling_rate):\n",
    "    # Load audio from the given path and set the sampling rate\n",
    "    X, sample_rate = librosa.load(audio_path ,res_type='kaiser_fast',duration=2.5,sr=sampling_rate*2,offset=0.5)\n",
    "    sample_rate = np.array(sample_rate)\n",
    "\n",
    "    # Separate harmonic and percussive components of the audio\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(X)\n",
    "    # Extract pitch and magnitudes of the audio\n",
    "    pitches, magnitudes = librosa.core.pitch.piptrack(y=X, sr=sample_rate)\n",
    "\n",
    "    # Extract the mean of the Mel-Frequency Cepstral Coefficients (MFCCs) of the audio\n",
    "    mfccs = np.mean(librosa.feature.mfcc(y=X,sr=sample_rate,n_mfcc=13),axis=1)\n",
    "\n",
    "    # Extract the mean of the pitches and remove trailing zeroes\n",
    "    pitches = np.trim_zeros(np.mean(pitches,axis=1))[:20]\n",
    "\n",
    "    # Extract the mean of the magnitudes and remove trailing zeroes\n",
    "    magnitudes = np.trim_zeros(np.mean(magnitudes,axis=1))[:20]\n",
    "\n",
    "    # Extract the mean of the chroma feature of the audio\n",
    "    chromas = np.mean(librosa.feature.chroma_cqt(y=y_harmonic, sr=sampling_rate),axis=1)\n",
    "    \n",
    "    # Return a list of features including the MFCCs, pitches, magnitudes, and chroma feature\n",
    "    return [mfccs, pitches, magnitudes, chromas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "273e8d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_audio_path = \"D:\\\\final_voice_model_3\\\\RAVDESS\\\\Actor_01\\\\03-01-01-01-01-01-01.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7f09c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the audio features (MFCC, pitch, magnitude, and chroma)\n",
    "demo_mfcc, demo_pitch, demo_mag, demo_chrom = extract_audio_features(demo_audio_path,20000)\n",
    "\n",
    "# Convert the audio features to Pandas Series\n",
    "mfcc = pd.Series(demo_mfcc)\n",
    "pit = pd.Series(demo_pitch)\n",
    "mag = pd.Series(demo_mag)\n",
    "C = pd.Series(demo_chrom)\n",
    "# Concatenate the audio features into a single dataframe\n",
    "demo_audio_features = pd.concat([mfcc,pit,mag,C],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d80e07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an additional dimension to the demo_audio_features array along the first axis (axis=0)\n",
    "demo_audio_features= np.expand_dims(demo_audio_features, axis=0)\n",
    "# Add another dimension to the demo_audio_features array along the second axis (axis=2)\n",
    "demo_audio_features= np.expand_dims(demo_audio_features, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2786965",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'model/flatten/Reshape' defined at (most recent call last):\n    File \"D:\\Anaconda\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\Anaconda\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"D:\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"D:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"D:\\Anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_5988\\2801692542.py\", line 2, in <module>\n      live_predictions = model.predict(demo_audio_features,\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\layers\\reshaping\\flatten.py\", line 104, in call\n      return tf.reshape(inputs, flattened_shape)\nNode: 'model/flatten/Reshape'\nInput to reshape is a tensor with 320 values, but the requested shape requires a multiple of 256\n\t [[{{node model/flatten/Reshape}}]] [Op:__inference_predict_function_611]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5988\\3182528382.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Use the loaded model to make predictions on the demo_audio_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlive_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdemo_audio_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model/flatten/Reshape' defined at (most recent call last):\n    File \"D:\\Anaconda\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"D:\\Anaconda\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"D:\\Anaconda\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"D:\\Anaconda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"D:\\Anaconda\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"D:\\Anaconda\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"D:\\Anaconda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_5988\\2801692542.py\", line 2, in <module>\n      live_predictions = model.predict(demo_audio_features,\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2253, in predict\n      tmp_batch_outputs = self.predict_function(iterator)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function\n      return step_function(self, iterator)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step\n      outputs = model.predict_step(data)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n      return self(x, training=False)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\", line 557, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\functional.py\", line 510, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\functional.py\", line 667, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1097, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"D:\\Anaconda\\lib\\site-packages\\keras\\layers\\reshaping\\flatten.py\", line 104, in call\n      return tf.reshape(inputs, flattened_shape)\nNode: 'model/flatten/Reshape'\nInput to reshape is a tensor with 320 values, but the requested shape requires a multiple of 256\n\t [[{{node model/flatten/Reshape}}]] [Op:__inference_predict_function_611]"
     ]
    }
   ],
   "source": [
    "# Use the loaded model to make predictions on the demo_audio_features\n",
    "live_predictions = model.predict(demo_audio_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a860e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2f6732",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions=[\"anger\",\"disgust\",\"fear\",\"happy\",\"neutral\", \"sad\", \"surprise\"]\n",
    "# Get the index of the emotion with the highest probability\n",
    "index = live_predictions.argmax(axis=1).item()\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa953af",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "649f41c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_5988\\2036098180.py:4: FutureWarning: Pass rate=0.7 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, rate)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_5988\\3778454608.py:4: FutureWarning: Pass sr=22050, n_steps=0.8 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_5988\\1913875087.py:4: FutureWarning: Pass rate=1.25 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, speed_factor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "sad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_5988\\1913875087.py:9: FutureWarning: Pass rate=0.75 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, speed_factor)\n"
     ]
    }
   ],
   "source": [
    "print(emotions[predict_mood(\"D:\\\\final_voice_model_3\\\\RAVDESS\\\\Actor_01\\\\03-01-01-01-01-01-01.wav\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88479ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0130dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91dc4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler\n",
    "scaler = StandardScaler()\n",
    "# scaler.mean_ = np.load('scaler_mean.npy')\n",
    "# scaler.scale_ = np.load('scaler_std.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0386283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the user input voice data\n",
    "audio_path = \"D:\\\\final_voice_model_3\\\\RAVDESS\\\\Actor_03\\\\03-01-03-02-02-02-03.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "917602ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['Neutral', 'Calm', 'Happy', 'Sad', 'Angry', 'Fearful', 'Disgust', 'Surprised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c7903ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_3700\\2036098180.py:4: FutureWarning: Pass rate=0.7 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, rate)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_3700\\3778454608.py:4: FutureWarning: Pass sr=22050, n_steps=0.8 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_3700\\1913875087.py:4: FutureWarning: Pass rate=1.25 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, speed_factor)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_3700\\1913875087.py:9: FutureWarning: Pass rate=0.75 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, speed_factor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3700\\2452829996.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;31m# Get the predicted emotion label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_emotion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memotions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpredicted_emotion_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Extract the features from the user input voice data\n",
    "features = get_features(path)\n",
    "\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(features)\n",
    "\n",
    "# Scale the features using the scaler\n",
    "scaled_features = scaler.transform(features)\n",
    "\n",
    "# Reshape the features for the model input\n",
    "reshaped_features = scaled_features.reshape(scaled_features.shape[0], scaled_features.shape[1], 1)\n",
    "\n",
    "# Predict the emotion label using the trained model\n",
    "prediction = model.predict(reshaped_features)\n",
    "\n",
    "# Get the index of the predicted emotion label\n",
    "predicted_emotion_index = np.argmax(prediction)\n",
    "\n",
    "# Get the predicted emotion label\n",
    "print(predicted_emotion = emotions[predicted_emotion_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf7d41e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_15468\\2036098180.py:4: FutureWarning: Pass rate=0.7 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, rate)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_15468\\3778454608.py:4: FutureWarning: Pass sr=22050, n_steps=0.8 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_15468\\1913875087.py:4: FutureWarning: Pass rate=1.25 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, speed_factor)\n",
      "C:\\Users\\Rashada\\AppData\\Local\\Temp\\ipykernel_15468\\1913875087.py:9: FutureWarning: Pass rate=0.75 as keyword args. From version 0.10 passing these as positional arguments will result in an error\n",
      "  return librosa.effects.time_stretch(data, speed_factor)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 168ms/step\n",
      "The predicted emotion is: sad\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model architecture from JSON file\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "\n",
    "# Load the saved model weights from H5 file\n",
    "model.load_weights('model.h5')\n",
    "# Load the model\n",
    "model = load_model('model.h5')\n",
    "# # Load the pre-trained model\n",
    "# model = load_model('model.h5')\n",
    "\n",
    "# Define the emotion labels\n",
    "emotion_labels = ['angry','calm','disgust','fear','happy','neutral',\n",
    "                'sad','surprise']\n",
    "\n",
    "# Define the path to the audio file\n",
    "audio_path = \"D:\\\\final_voice_model\\\\RAVDESS\\\\Actor_01\\\\03-01-08-02-02-01-01.wav\"\n",
    "\n",
    "# Get the features for the audio file\n",
    "features = get_features(audio_path)\n",
    "\n",
    "# Reshape the features to have the same shape as the input to the model\n",
    "features = np.reshape(features, (features.shape[0], features.shape[1], 1))\n",
    "\n",
    "# Make predictions using the model\n",
    "predictions = model.predict(features)\n",
    "\n",
    "# Get the index of the predicted emotion label\n",
    "predicted_label_index = np.argmax(predictions[0])\n",
    "\n",
    "# Get the predicted emotion label\n",
    "predicted_label = emotion_labels[predicted_label_index]\n",
    "\n",
    "# Print the predicted emotion label\n",
    "print('The predicted emotion is:', predicted_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365c925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
