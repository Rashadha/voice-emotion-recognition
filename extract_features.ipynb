{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87d4a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from adding_augmentation.ipynb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "# to import .ipynb files\n",
    "import import_ipynb\n",
    "import adding_augmentation\n",
    "import os # for interacting with the operating system\n",
    "import numpy as np # for linear algebra operations\n",
    "import pandas as pd # for data processing and reading/writing CSV files\n",
    "import matplotlib.pyplot as plt # for plotting data\n",
    "import seaborn as sns # for creating beautiful plots\n",
    "import librosa # a library for analyzing audio and music\n",
    "import librosa.display # for displaying audio data\n",
    "from IPython.display import Audio # for playing audio files\n",
    "\n",
    "# Importing necessary modules from scikit-learn library\n",
    "# StandardScaler is used for feature scaling\n",
    "# OneHotEncoder is used for one-hot encoding categorical variables\n",
    "# confusion_matrix and classification_report are used for evaluating the performance of a classifier\n",
    "# train_test_split is used for splitting the data into training and testing sets\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, AveragePooling1D\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, AveragePooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1867c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract the features from audio data\n",
    "def extract_features(data):\n",
    "    result = np.array([])\n",
    "    \n",
    "    # Compute the Mel-frequency cepstral coefficients (MFCCs)\n",
    "    # Use 58 MFCCs to get ~60 ms frames\n",
    "    mfccs = librosa.feature.mfcc(y=data, sr=22050, n_mfcc=58)\n",
    "    \n",
    "    # Compute the average MFCCs for each frame\n",
    "    mfccs_processed = np.mean(mfccs.T,axis=0)\n",
    "    \n",
    "    # Add the processed MFCCs to the result array\n",
    "    result = np.array(mfccs_processed)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8869f873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get features for a given audio file\n",
    "def get_features(path):\n",
    "    # Load the audio file, taking care of the no audio at the start and end of the file\n",
    "    data, sample_rate = librosa.load(path, duration=3, offset=0.5, res_type='kaiser_fast') \n",
    "    \n",
    "    # Extract features without augmentation\n",
    "    result_1 = extract_features(data)\n",
    "    result = np.array(result_1)\n",
    "    \n",
    "    # Extract features with added noise\n",
    "    noise_data = adding_augmentation.add_noise(data)\n",
    "    result_2 = extract_features(noise_data)\n",
    "    result = np.vstack((result, result_2)) # Stack the results vertically\n",
    "    \n",
    "    # Extract features with time stretching\n",
    "    stretch_data = adding_augmentation.stretch(data)\n",
    "    result_3 = extract_features(stretch_data)\n",
    "    result = np.vstack((result, result_3))\n",
    "    \n",
    "    # Extract features with time shifting\n",
    "    shift_data = adding_augmentation.shift(data)\n",
    "    result_4 = extract_features(shift_data)\n",
    "    result = np.vstack((result, result_4))\n",
    "    \n",
    "    # Extract features with pitch shifting\n",
    "    pitch_data = adding_augmentation.change_pitch(data, sample_rate)\n",
    "    result_5 = extract_features(pitch_data)\n",
    "    result = np.vstack((result, result_5)) \n",
    "    \n",
    "    # Extract features with increased speed\n",
    "    higher_speed_data = adding_augmentation.increase_speed(data)\n",
    "    result_6 = extract_features(higher_speed_data)\n",
    "    result = np.vstack((result, result_6))\n",
    "    \n",
    "    # Extract features with decreased speed\n",
    "    lower_speed_data = adding_augmentation.decrease_speed(data)\n",
    "    result_7 = extract_features(lower_speed_data)\n",
    "    result = np.vstack((result, result_7))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eabeada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process1(females,males):\n",
    "    # Initializing empty arrays for female data\n",
    "    female_X, female_Y = [], []\n",
    "    \n",
    "    # Looping through each path and emotion of Female datase\n",
    "    for path, emotion in zip(females.path, females.labels):\n",
    "        features = get_features(path)\n",
    "        # Loop through the augmented features returned from get_features()\n",
    "        #adding augmentation, get_features return a multi dimensional array (for each augmentation), so we have to use a loop to fill the df\n",
    "        for element in features: \n",
    "            female_X.append(element)        \n",
    "            female_Y.append(emotion)\n",
    "    \n",
    "\n",
    "    # Initializing empty arrays for male data\n",
    "    male_X, male_Y = [], []\n",
    "    \n",
    "    # Looping through each path and emotion of Male dataset\n",
    "    for path, emotion in zip(males.path, males.labels):\n",
    "        features = get_features(path)\n",
    "        for element in features:\n",
    "            male_X.append(element)\n",
    "            male_Y.append(emotion)\n",
    "            \n",
    "    # Print the shapes of female and male data arrays\n",
    "    print('Female features: ',len(female_X),' labels: ',len(female_Y))\n",
    "    print('Male features: ',len(male_X),' labels: ',len(male_Y))\n",
    "    return female_X, female_Y, male_X, male_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c87c1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataframe(gender, features, labels):\n",
    "    # Create a dataframe from the extracted features\n",
    "    data_frame = pd.DataFrame(features)\n",
    "    \n",
    "    # Add the labels column to the dataframe\n",
    "    data_frame['labels'] = labels\n",
    "    \n",
    "    # Save the dataframe as a CSV file with the given gender name\n",
    "    data_frame.to_csv(f'D:\\\\final_voice_model_3\\\\features\\\\{gender}_features.csv', index=False)\n",
    "    \n",
    "    # Print the sample of the dataframe\n",
    "    print(f'{gender} dataframe')\n",
    "    data_frame.sample(frac=1).head()\n",
    "    \n",
    "    # Return the created dataframe\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c0b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process2(female_X, female_Y, male_X, male_Y):\n",
    "    print()\n",
    "    Females_Features = setup_dataframe('female', female_X, female_Y)\n",
    "    Males_Features = setup_dataframe('male', male_X, male_Y)\n",
    "    return Females_Features, Males_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e400ca46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
